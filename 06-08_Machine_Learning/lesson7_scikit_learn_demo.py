# %%
"""
Lesson 7: Scikit-learn æ©Ÿå™¨å­¸ç¿’å¥—ä»¶å¯¦éš›æ“ä½œç·´ç¿’
===================================================

æœ¬ç¯„ä¾‹ä»‹ç´¹å¦‚ä½•ä½¿ç”¨ Scikit-learn é€²è¡Œåˆ†é¡èˆ‡å›æ­¸æ¨¡å‹çš„å»ºç«‹èˆ‡è©•ä¼°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer, make_classification, make_regression
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.svm import SVC, SVR
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Set up matplotlib parameters
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("Lesson 7: Scikit-learn æ©Ÿå™¨å­¸ç¿’å¥—ä»¶å¯¦éš›æ“ä½œç·´ç¿’")
print("=" * 60)

# %%

# ============================================================================
# Part 1: åˆ†é¡å•é¡Œ (Classification) - ä½¿ç”¨ä¹³è…ºç™Œè³‡æ–™é›†
# ============================================================================

print("\nğŸ“Š Part 1: åˆ†é¡å•é¡Œ - ä¹³è…ºç™Œè³‡æ–™é›†")
print("-" * 50)

# è¼‰å…¥ä¹³è…ºç™Œè³‡æ–™é›†
cancer = load_breast_cancer()
X_cancer = cancer.data
y_cancer = cancer.target

# å°‡è³‡æ–™è½‰æ›ç‚º DataFrame æ–¹ä¾¿æ“ä½œ
cancer_df = pd.DataFrame(X_cancer, columns=cancer.feature_names)
cancer_df['diagnosis'] = y_cancer
cancer_df['diagnosis_name'] = cancer_df['diagnosis'].map({0: 'malignant', 1: 'benign'})

print(f"è³‡æ–™å½¢ç‹€: {cancer_df.shape}")
print(f"ç‰¹å¾µæ•¸é‡: {len(cancer.feature_names)}")
print(f"åˆ†é¡æ¨™ç±¤: {cancer.target_names}")
print(f"æƒ¡æ€§è…«ç˜¤æ¨£æœ¬æ•¸: {(y_cancer == 0).sum()}")
print(f"è‰¯æ€§è…«ç˜¤æ¨£æœ¬æ•¸: {(y_cancer == 1).sum()}")
print("\nè³‡æ–™é›†æè¿°:")
print("å¨æ–¯åº·è¾›ä¹³è…ºç™Œè³‡æ–™é›†åŒ…å«569å€‹æ¨£æœ¬å’Œ30å€‹ç‰¹å¾µ")
print("ç‰¹å¾µåŒ…æ‹¬ç´°èƒæ ¸çš„åŠå¾‘ã€è³ªåœ°ã€å‘¨é•·ã€é¢ç©ã€å¹³æ»‘åº¦ç­‰")
print("ç›®æ¨™æ˜¯é æ¸¬è…«ç˜¤æ˜¯æƒ¡æ€§(malignant=0)é‚„æ˜¯è‰¯æ€§(benign=1)")
print("\nå‰5ç­†è³‡æ–™:")
print(cancer_df.head())

# %%
# è³‡æ–™è¦–è¦ºåŒ–
plt.figure(figsize=(18, 12))

# Select some important features for visualization
important_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness']

# 1. Diagnosis distribution
plt.subplot(2, 4, 1)
cancer_df['diagnosis_name'].value_counts().plot(kind='bar')
plt.title('Diagnosis Distribution')
plt.ylabel('Number of Samples')

# 2. Important features boxplot
plt.subplot(2, 4, 2)
sns.boxplot(data=cancer_df.melt(id_vars=['diagnosis_name'], 
                                value_vars=important_features[:3]), 
            x='variable', y='value', hue='diagnosis_name')
plt.title('Important Features Distribution')
plt.xticks(rotation=45)

# 3-5. Important features scatter plots
for i, feature in enumerate(important_features[:3]):
    plt.subplot(2, 4, i+3)
    sns.scatterplot(data=cancer_df, x=feature, y='mean area', 
                    hue='diagnosis_name', s=30, alpha=0.7)
    plt.title(f'{feature} vs Mean Area')

# 6. Correlation heatmap of first 10 features
plt.subplot(2, 4, 6)
correlation = cancer_df[cancer.feature_names[:10]].corr()
sns.heatmap(correlation, annot=False, cmap='coolwarm', center=0)
plt.title('First 10 Features Correlation')

# 7. Feature importance (using simple statistics)
plt.subplot(2, 4, 7)
feature_means = cancer_df.groupby('diagnosis_name')[important_features].mean()
feature_diff = abs(feature_means.loc['malignant'] - feature_means.loc['benign'])
feature_diff.plot(kind='bar')
plt.title('Malignant vs Benign Feature Difference')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# %%
# Split training and testing data
X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(
    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer
)

print(f"\nè¨“ç·´è³‡æ–™å½¢ç‹€: {X_train_cancer.shape}")
print(f"æ¸¬è©¦è³‡æ–™å½¢ç‹€: {X_test_cancer.shape}")

# Feature standardization
scaler_cancer = StandardScaler()
X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)
X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)

# %%
# Build multiple classification models
classifiers = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42)
}

classification_results = {}

print("\nğŸ¤– è¨“ç·´åˆ†é¡æ¨¡å‹...")
for name, clf in classifiers.items():
    print(f"\n{name}:")
    
    # Train model
    if name == 'SVM':
        clf.fit(X_train_cancer_scaled, y_train_cancer)
        y_pred = clf.predict(X_test_cancer_scaled)
    else:
        clf.fit(X_train_cancer, y_train_cancer)
        y_pred = clf.predict(X_test_cancer)
    
    # Evaluate model
    accuracy = accuracy_score(y_test_cancer, y_pred)
    cv_scores = cross_val_score(clf, X_train_cancer if name != 'SVM' else X_train_cancer_scaled, 
                                y_train_cancer, cv=5)
    
    classification_results[name] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"  æ¸¬è©¦æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"  äº¤å‰é©—è­‰å¹³å‡: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# %%
# Visualize classification results
plt.figure(figsize=(15, 5))

for i, (name, results) in enumerate(classification_results.items()):
    plt.subplot(1, 3, i+1)
    cm = confusion_matrix(y_test_cancer, results['predictions'])
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=cancer.target_names,
                yticklabels=cancer.target_names)
    plt.title(f'{name}\nAccuracy: {results["accuracy"]:.4f}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

plt.tight_layout()
plt.show()

# %%
# ============================================================================
# Part 2: å›æ­¸å•é¡Œ (Regression) - ä½¿ç”¨åˆæˆè³‡æ–™é›†
# ============================================================================

print("\n\nğŸ“ˆ Part 2: å›æ­¸å•é¡Œ - åˆæˆè³‡æ–™é›†")
print("-" * 50)

# Create synthetic regression dataset
X_reg, y_reg = make_regression(n_samples=500, n_features=5, noise=0.1, random_state=42)

# Convert to DataFrame
feature_names = [f'feature_{i+1}' for i in range(X_reg.shape[1])]
reg_df = pd.DataFrame(X_reg, columns=feature_names)
reg_df['target'] = y_reg

print(f"å›æ­¸è³‡æ–™å½¢ç‹€: {reg_df.shape}")
print("\nå‰5ç­†è³‡æ–™:")
print(reg_df.head())

# %%
# Regression data visualization
plt.figure(figsize=(15, 8))

# 1. Feature vs target relationship
for i in range(5):
    plt.subplot(2, 3, i+1)
    plt.scatter(reg_df[f'feature_{i+1}'], reg_df['target'], alpha=0.6)
    plt.xlabel(f'Feature {i+1}')
    plt.ylabel('Target Value')
    plt.title(f'Feature {i+1} vs Target Value')

# 6. Correlation heatmap
plt.subplot(2, 3, 6)
correlation = reg_df.corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation')

plt.tight_layout()
plt.show()

# %%
# Split training and testing data
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

# Feature standardization
scaler_reg = StandardScaler()
X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)
X_test_reg_scaled = scaler_reg.transform(X_test_reg)

# Build multiple regression models
regressors = {
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'SVM': SVR(kernel='rbf')
}

regression_results = {}

print("\nğŸ¤– è¨“ç·´å›æ­¸æ¨¡å‹...")
for name, reg in regressors.items():
    print(f"\n{name}:")
    
    # Train model
    if name == 'SVM':
        reg.fit(X_train_reg_scaled, y_train_reg)
        y_pred = reg.predict(X_test_reg_scaled)
    else:
        reg.fit(X_train_reg, y_train_reg)
        y_pred = reg.predict(X_test_reg)
    
    # Evaluate model
    mse = mean_squared_error(y_test_reg, y_pred)
    mae = mean_absolute_error(y_test_reg, y_pred)
    r2 = r2_score(y_test_reg, y_pred)
    
    regression_results[name] = {
        'mse': mse,
        'mae': mae,
        'r2': r2,
        'predictions': y_pred
    }
    
    print(f"  å‡æ–¹èª¤å·® (MSE): {mse:.4f}")
    print(f"  å¹³å‡çµ•å°èª¤å·® (MAE): {mae:.4f}")
    print(f"  RÂ² åˆ†æ•¸: {r2:.4f}")

# %%
# Visualize regression results
plt.figure(figsize=(15, 5))

for i, (name, results) in enumerate(regression_results.items()):
    plt.subplot(1, 3, i+1)
    plt.scatter(y_test_reg, results['predictions'], alpha=0.6)
    plt.plot([y_test_reg.min(), y_test_reg.max()], 
             [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title(f'{name}\nRÂ² = {results["r2"]:.4f}')

plt.tight_layout()
plt.show()

# %%
# ============================================================================
# Part 3: è¶…åƒæ•¸èª¿å„ªç¤ºä¾‹
# ============================================================================

print("\n\nâš™ï¸ Part 3: è¶…åƒæ•¸èª¿å„ªç¤ºä¾‹")
print("-" * 50)

# Use Random Forest for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10]
}

rf_classifier = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

print("ğŸ” é€²è¡Œç¶²æ ¼æœç´¢...")
grid_search.fit(X_train_cancer, y_train_cancer)

print(f"æœ€ä½³åƒæ•¸: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰é©—è­‰åˆ†æ•¸: {grid_search.best_score_:.4f}")

# Evaluate model with best parameters
best_rf = grid_search.best_estimator_
y_pred_best = best_rf.predict(X_test_cancer)
best_accuracy = accuracy_score(y_test_cancer, y_pred_best)

print(f"å„ªåŒ–å¾Œæ¸¬è©¦æº–ç¢ºç‡: {best_accuracy:.4f}")

# %%
# ============================================================================
# Summary Report
# ============================================================================

print("\n\nğŸ“‹ æ¨¡å‹æ•ˆèƒ½ç¸½çµå ±å‘Š")
print("=" * 60)

print("\nåˆ†é¡æ¨¡å‹çµæœ:")
print("-" * 30)
for name, results in classification_results.items():
    print(f"{name:20s} - æº–ç¢ºç‡: {results['accuracy']:.4f}")

print("\nå›æ­¸æ¨¡å‹çµæœ:")
print("-" * 30)
for name, results in regression_results.items():
    print(f"{name:20s} - RÂ²: {results['r2']:.4f}, MSE: {results['mse']:.4f}")

print(f"\nè¶…åƒæ•¸å„ªåŒ–çµæœ:")
print("-" * 30)
print(f"Random Forest åŸå§‹æº–ç¢ºç‡: {classification_results['Random Forest']['accuracy']:.4f}")
print(f"Random Forest å„ªåŒ–å¾Œæº–ç¢ºç‡: {best_accuracy:.4f}")
print(f"æ”¹å–„ç¨‹åº¦: {((best_accuracy - classification_results['Random Forest']['accuracy']) * 100):.2f}%")

print("\nğŸ“ å­¸ç¿’é‡é»ç¸½çµ:")
print("1. Scikit-learn æä¾›äº†å®Œæ•´çš„æ©Ÿå™¨å­¸ç¿’å·¥å…·çµ„")
print("2. ä¹³è…ºç™Œè³‡æ–™é›†æ˜¯ç¶“å…¸çš„äºŒå…ƒåˆ†é¡å•é¡Œ (æƒ¡æ€§/è‰¯æ€§)")
print("3. é«˜ç¶­è³‡æ–™é›† (30å€‹ç‰¹å¾µ) éœ€è¦é©ç•¶çš„ç‰¹å¾µé¸æ“‡å’Œè¦–è¦ºåŒ–")
print("4. ç‰¹å¾µæ¨™æº–åŒ–å°æŸäº›æ¼”ç®—æ³•å¾ˆé‡è¦")
print("5. äº¤å‰é©—è­‰å¯ä»¥æ›´å¥½åœ°è©•ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›")
print("6. è¶…åƒæ•¸èª¿å„ªå¯ä»¥é€²ä¸€æ­¥æå‡æ¨¡å‹æ•ˆèƒ½")
print("7. è¦–è¦ºåŒ–æ˜¯ç†è§£æ¨¡å‹çµæœçš„é‡è¦å·¥å…·")

print("\nâœ… Scikit-learn åŸºç¤æ“ä½œç·´ç¿’å®Œæˆï¼") 